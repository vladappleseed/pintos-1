 +			+--------------------+
 +			| PINTOS             |
 +			| PROJECT 1: THREADS |
 +			| DESIGN DOCUMENT    |
 +			+--------------------+
 +				   
 +---- GROUP ----
 +
 +>> Fill in the names and email addresses of your group members.
 +
 +Anthony Areniego <anthony.areniego@ucdenver.edu>
 +Bill Adona <bill.adona@ucdenver.edu>
 +Vladislav Makarov <vladislav.makarov@ucdenver.edu>
 +Andrew Bentley <andrew.bentley@ucdenver.edu>
 +
 +
 +			     ALARM CLOCK
 +			     ===========
 +
 +---- DATA STRUCTURES ----
 +
 +  struct list sleep_list;
 +	- List that keeps track of processes that are sleeping
 +
 +---- ALGORITHMS ----
 +
 +>> A2: Briefly describe what happens in a call to timer_sleep(),
 +>> including the effects of the timer interrupt handler.
 +	-In timer_sleep()
	1. Check for valid ticks argument
	2. Disable the interrupt so thread can be put to sleep
	3. Add the timer ticks into data member of thread to track ticks
	4. Add the current thread into the sleep_list queue for sleeping threads
	5. Put the thread to sleep
		
 +>> A3: What steps are taken to minimize the amount of time spent in
 +>> the timer interrupt handler?
	The sleep list is in sorted order which allows the for the handler to not have
	to go through the entire sleep list at every interrupt.	
 +
 +---- SYNCHRONIZATION ----
 +
 +>> A4: How are race conditions avoided when multiple threads call
 +>> timer_sleep() simultaneously?
	The check at the start of timer_sleep only allows one thread to continue onto
	the further steps. Disabling the interrupt prevents another thread from
	continuing.
 +
 +>> A5: How are race conditions avoided when a timer interrupt occurs
 +>> during a call to timer_sleep()?
	Since ticks in timer_sleep is a local variable, it doesnt allow a race 
	condition. Also if the ticks value is invalid, the thread doesnt continue.
 +
 +---- RATIONALE ----
 +
 +>> A6: Why did you choose this design?  In what ways is it superior to
 +>> another design you considered?
	This design was the easiest for all group members to understand and implement.
 +
 +			 PRIORITY SCHEDULING
 +			 ===================
 +
 +---- DATA STRUCTURES ----
 +
 +>> B2: Explain the data structure used to track priority donation.
 +>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
 +>> .png file.)

A, B, C are locks
H1, H2, M, L are threads

    A
H1 --->    C
H2 ---> M ---> L
    B

M's donation list: H1, H2
L's donation list: M

M's wait on lock: C
L's wait on lock: NULL

M's current donated priority is max(H1, H2, M).
L's current donated priority is max(L, M).
 +
 +---- ALGORITHMS ----
 +
 +>> B3: How do you ensure that the highest priority thread waiting for
 +>> a lock, semaphore, or condition variable wakes up first?
	The list of waiters lines up with the lists of priority threads so when 
	a waiter is inserted, it is the highest priority with the threads.
 +
 +>> B4: Describe the sequence of events when a call to lock_acquire()
 +>> causes a priority donation.  How is nested donation handled?
	1) The current thread updates its wait on lock variable to the current
	lock.
	2) The current thread adds itself to the lock holder's donations list.
	3) Priority is donated iteratively via the following process (written
	   in pseudocode):

	- thread = current thread
   	- lock = lock current thread is waiting on
	   while (lock exists)	
	   {
	     - if there is no holder for lock, return
	     - if the lock holder has a bigger (or equal) priority than thread's
	       priority, return
 	    - Set the lock holder's priority = thread's priority
 	    - Update thread = lock holder
 	    - Update lock = lock thread is waiting on
 	  }
 +
 +>> B5: Describe the sequence of events when lock_release() is called
 +>> on a lock that a higher-priority thread is waiting for.
	1. The lock holder is NULL
	2. The priority donation is gone when the lock is released
	3. Find the highest priority thread on the donation list
	4. The thread acquires the lock
	5. Check for max priority
 +
 +---- SYNCHRONIZATION ----
 +
 +>> B6: Describe a potential race in thread_set_priority() and explain
 +>> how your implementation avoids it.  Can you use a lock to avoid
 +>> this race?
	A potential race condition would be while the thread priority variable is
	being updated to the new priority, the interrupt handler is writing to the
	priority variable. Thus, these conflicting writes could mangle the
	priority variable, therefore being a race condition.
 +
 +---- RATIONALE ----
 +
 +>> B7: Why did you choose this design?  In what ways is it superior to
 +>> another design you considered?
 +
 +			  ADVANCED SCHEDULER
 +			  ==================
 +
 +---- DATA STRUCTURES ----
 +
 +>> C1: Copy here the declaration of each new or changed `struct' or
 +>> `struct' member, global or static variable, `typedef', or
 +>> enumeration.  Identify the purpose of each in 25 words or less.
 +
 +---- ALGORITHMS ----
 +
 +>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
 +>> has a recent_cpu value of 0.  Fill in the table below showing the
 +>> scheduling decision and the priority and recent_cpu values for each
 +>> thread after each given number of timer ticks:
 +
Assume time slice = 4 ticks.

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run    ready list
-----  --  --  --  --  --  --   ------    ----------
 0     0   0   0   63  61  59     A          B, C
 4     4   0   0   62  61  59     A          B, C
 8     8   0   0   61  61  59     B          A, C
12     8   4   0   61  60  59     A          B, C
16     12  4   0   60  60  59     B          A, C
20     12  8   0   60  59  59     A          C, B
24     16  8   0   59  59  59     C          B, A
28     16  8   4   59  59  58     B          A, C
32     16  12  4   59  58  58     A          C, B
36     20  12  4   58  58  58     C          B, A
 +
 +>> C3: Did any ambiguities in the scheduler specification make values
 +>> in the table uncertain?  If so, what rule did you use to resolve
 +>> them?  Does this match the behavior of your scheduler?
	There is an ambiguity when two threads have the same priority. In order
	to resolve it, the currently running thread will continue or the thread
	least recently used will run first.
 +
 +>> C4: How is the way you divided the cost of scheduling between code
 +>> inside and outside interrupt context likely to affect performance?
	Since most of the computations are done inside the interrupt handler,
	performance outside the interrupt context is efficient. 
 +
 +---- RATIONALE ----
 +
 +>> C5: Briefly critique your design, pointing out advantages and
 +>> disadvantages in your design choices.  If you were to have extra
 +>> time to work on this part of the project, how might you choose to
 +>> refine or improve your design?
	Advantages
	- Simple
	- Small use of new variables
	Disadvantages
	- Some performance is lost because of how interrupts are handled
	- Sorting and searching through the lists takes away from performance
		as well
	
